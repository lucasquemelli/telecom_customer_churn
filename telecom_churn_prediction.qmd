# 1. Loading Data

```{r}
setwd("/Users/lucasquemelli/Documents/repos/telecom_customer_churn")
#source ("/Users/lucasquemelli/Documents/repos/telecom_customer_churn/data_ingestion.R")
#source ("/Users/lucasquemelli/Documents/repos/telecom_customer_churn/exploratory_data_analysis.R")
#source ("/Users/lucasquemelli/Documents/repos/telecom_customer_churn//pre_processing.R")

set.seed(007)
churn_data_raw = read.csv("/Users/lucasquemelli/Documents/repos/telecom_customer_churn/data/churn.csv")
```

```{r}
churn_data_raw
```

```{r}
View(churn_data_raw)
```

# 2. Helper Functions

## Installing Packages

If the packages are not in the system, go ahead and install it and its dependencies. Then open it.

```{r}
### First specify the packages of interest
packages = c("tidyverse", "utils","mice","dplyr",'lime','keras','ggplot2','ggthemes','tidyquant','rsample','recipes','yardstick','corrr','readr','tidyr', "phytools", "viridis")

## Now load or install & load all
package.check <- lapply(
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
)
```

```{r}
library("tidyverse")
library("utils")
library("mice")
library("dplyr")
library("lime")
library("keras")
library("ggplot2")
library("ggthemes")
library("tidyquant")
library("rsample")
library("recipes")
library("yardstick")
library("corrr")
library("readr")
library("tidyr")
library("phytools")
library("viridis")
```

```{r}
#install.packages("tensorflow")
library(tensorflow)
library(reticulate)
#path_to_python <- install_python()
#virtualenv_create("r-reticulate", python = path_to_python)
#install_tensorflow(envname = "r-reticulate")
```

```{r}
library(tensorflow)
```

```{r}
#install.packages("keras")
library(keras)
#install_keras(envname = "r-reticulate")
```

```{r}
#install.packages("tibble")
library(tibble)
```

```{r}
#install.packages("MLmetrics")
library(MLmetrics)
library(dplyr)
```

## Numerical Variables

```{r}
### Visualizing Numerical  Variables ###
plot_num_var <- function (churn_data_raw){
churn_data_raw %>%
  gather(x, y, -Churn) %>%
  ggplot(aes(x = y, fill = Churn, color = Churn)) +
  facet_wrap(~ x, ncol = 3, scales = "free") +
  geom_density(alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "top") +
  scale_color_tableau() +
  scale_fill_tableau()

}
```

## Identifying and removing missing values

1.  We use the mice package to find out any sort of patterns that exists for the missing data values.

```{r}
### Pre-Processing data
missing_data_snapshot <- function (churn_data_raw){
  churn_data <- churn_data_raw %>%
    select(-customerID)
  mice::md.pattern(churn_data, plot = FALSE)
}

### Dropping those missing values
preprocess_data <- function (churn_data_raw){
  churn_data <- churn_data_raw %>%
    select(-customerID)
  mice::md.pattern(churn_data, plot = FALSE)
  churn_data <- churn_data %>%
    drop_na()
  return(churn_data)
}
```

## Training and test datasets split

```{r}
### Creating train data
train_data <- function(churn_data_raw, train_proportion){
  churn_data_tbl <- churn_data_raw %>%
    select(-customerID) %>%
    drop_na() %>%
    select(Churn, everything())
  glimpse(churn_data_tbl)

  # Split test/training sets
  train_test_split <- initial_split(churn_data_tbl, prop = train_proportion)
  train_test_split
  return(train_test_split)
}
```

## Categorical Variables

```{r}
### Visualizing Categorical Variables ###
plot_cat_var<- function (churn_data_raw){

  churn_data_raw %>%
    select(-customerID) %>%
    select_if(is.character) %>%
    select(Churn, everything()) %>%
    gather(x, y, gender:PaymentMethod) %>% #select columns from gender to PaymentMethod
    count(Churn, x, y) %>%
    ggplot(aes(x = y, y = n, fill = Churn, color = Churn)) +
    facet_wrap(~ x, ncol = 4, scales = "free") +
    geom_bar(stat = "identity", alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 9, hjust = 1),
          legend.position = "top") +
    scale_color_tableau() +
    scale_fill_tableau()
}
```

## Creating Recipe

1.  We selected the target variable and the dataset using recipe function.
2.  Tenure is a continuous variable. We used step_discretize function in order to make the variable tenure assume discrete values.
3.  We have made log transformation into the TotalCharges variable due to its broad range.
4.  We made dummies in order to convert categorical variables into binaries 1 and 0, then transforming them to columns. In step_dummy, we selected the categorical variables using all_nominal() and excluded the target variable using -all_outcomes().
5.  We created the mean center using step_center function. This step is to normalize our data.
6.  We used step_scale function in order to scale our data. Since we have variables in different scales, it is important to scale them in order to have a better performance training the model.
7.  Finally, we used prep function to estimate the required parameters from the training set.

```{r}
# Create recipe for performing some pre-processsing steps on the data
create_recipe <- function (train_tbl){
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
  step_discretize(tenure, options = list(cuts = 6)) %>%
  step_log(TotalCharges) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train_tbl)
  return(rec_obj)
}
```

# 3. Exploratory Data Analysis

## 3.1. Numerical variables bivariate analysis

Below, the density plots of the numerical variables.

-   Here we select only the numerical variables. We input the column name of each numerical variable.

-   Then, we use the function defined in the **Helper Function** section to plot density plots with the numerical variables selected previously.

```{r}
### Selecting Numerical variables to visualize
num_variables <- c("Churn", "MonthlyCharges", "tenure", "TotalCharges")
num_var_data  <-  churn_data_raw[num_variables]

### Bi-variate analysis  

#Visualize the numerical data
plot_num_var(num_var_data)
```

-   For higher tenure (months the user has stayed with the company), the less is the churn.

-   As the monthly charges (the amount that is charged to the customer every month) increases, the churn also increases.

## 3.2. Categorical variables bivariate analysis

Below, the stacked histograms of the categorical variables.

-   We select only the variables which are of char type.

-   Then, we use the function defined in the **Helper Functions** section to plot the stacked histograms below.

```{r}
#Visualize the categorical data
cat_var_data=churn_data_raw[,sapply(churn_data_raw,is.character) ]
plot_cat_var(cat_var_data)
```

-   There are more users who does not have dependents and also a higher number of churn.

-   Contracts of long periods have lower number of churn.

-   Users with has no online security has a higher number of churn.

-   Users with paperless billing has a higher number of churn.

-   Users with payment method equals to electronic check has a higher number of churn.

-   Users with phone service has a higher number of churn.

-   Users with no tech support has a higher number of churn.

# 4. Data Pre-processing

## 4.1 Missing values check

1.  Firstly, we used the **missing_data_snapshot** function.
2.  Since we have found only 11 rows with missing values for 7032 in total, we chose remove all of them.

```{r}
# Missing values snapshot
missing_data_snapshot(churn_data_raw)

# Missing values removal
x <- preprocess_data(churn_data_raw)
```

-   Only TotalCharges has missing values. The number of missing values is 11.

Since the percentage of missing values is very low, we removed all of them.

```{r}
# Percentage of missing values
100*11/7032
```

## 4.2 Training and test split

We split the dataset into training and test and then created a recipe using the functions in the **Helper Functions** section.

```{r}
### Creating train train split with 80% data in train
train_test_split <- train_data(churn_data_raw, .8) ### Input data & % of data for training

# Partition train and test dataset
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)

### Creating Recipe Object
rec_obj<- create_recipe(train_tbl)
rec_obj
```

We baked the recipe by creating our relevant test and training data for the predictor variables, based on the transformations made on the recipe object.

1.  We used the bake function to the with train_tbl the same the is found in the recipe object. Then, we excluded the target variable.
2.  Sequentially, we also used bake for the test dataset.
3.  glimpse() is **like a transposed version of print()** : columns run down the page, and data runs across.

```{r}
### Baking the recipes
# Predictors
x_train_tbl <- bake(rec_obj, new_data = as.data.frame(train_tbl)) %>% select(-Churn)
x_test_tbl  <- bake(rec_obj, new_data = test_tbl) %>% select(-Churn)
glimpse(x_train_tbl)
```

We also prepared the target variable for training and test datasets. For the both datasets, what we did means: if the variable Churn is equal to "Yes", then attribute to it the valor 1, else 0.

```{r}
# Response variables for training and testing sets
y_train_vec <- ifelse(pull(train_tbl, Churn) == "Yes", 1, 0)
y_test_vec  <- ifelse(pull(test_tbl, Churn) == "Yes", 1, 0)
glimpse(y_train_vec)
```

# 5. Machine Learning Modelling

## 5.1. Training the model

We trained a neural network MLP (Multi-Layer Perceptron), where:

-   We created (1) the first layer, (2) two hidden layers and (3) the last layer.

-   For the first layer, the number of neurons is the next 2ˆn the is higher than the number of the train set (x_train_tbl) dimensions. As the number of dimensions for x_train_tbl is 20, the next 2ˆn that is higher than 20 is 2ˆ5 = 32.

-   The following hidden layers have the number of neurons equals to the half of the previous layer.

-   The number of neurons for the last layer is the number of dimensions for the response variable. In this case, it is 1.

-   kernel_initializer is the initialization schemes that create the layer's weights. This initializer generates tensors with a uniform distribution.

-   The **rectified linear activation function** or **ReLU** is a linear function that will output the input directly if it is positive, otherwise, it will output zero. It has become the default activation function for many types of neural networks because a model that uses it is easier to train and often achieves better performance. It is ideal for the first layers.

-   We used sigmoid for the last layer since it is a classification problem and it is a binary-class output type.

-   Dropout consists in randomly setting a fraction **`rate`** of input units to a valor at each update during training time, which helps prevent overfitting. The rate is a float between 0 and 1; it is the fraction of the input units to drop.

-   Finally, the optimizer is adamax (ideal for classification problems) and we are looking at binary_accuracy since we have a binary-class output type.

```{r}
### Creating sequential model using Keras
dev_keras_model<- function(x_train_tbl){
  model_keras <- keras_model_sequential()

  model_keras %>%
    layer_dense(units = 32, kernel_initializer = "uniform", activation = "relu",
                input_shape = ncol(x_train_tbl)) %>%
    layer_dropout(rate = 0.2) %>%

    layer_dense(units = 16, kernel_initializer = "uniform", activation = "relu") %>%
    layer_dropout(rate = 0.2) %>%

    layer_dense(units = 8, kernel_initializer = "uniform", activation = "relu") %>%
    layer_dropout(rate = 0.2) %>%

    layer_dense(units = 1,
                kernel_initializer = "uniform", activation = "sigmoid") %>%

    compile(
      optimizer = 'adamax',
      loss      = 'binary_crossentropy',
      metrics   = c("binary_accuracy", "mse")
    )

  return(model_keras)

}
```

```{r}
#######  Building Keras Model#######
model_keras <- dev_keras_model(x_train_tbl)

### Viewing the model layers
model_keras
```

## 5.2. Model Fitting

**`validation_split()`** takes a single random sample (without replacement) of the original data set to be used for analysis. All other data points are added to the assessment set (to be used as the validation set). This is a manner of using different portions of our dataset in order to achieve a good variability during fitting (similar to Cross-Validation).

```{r}
fit_model <- function(x_train_tbl, y_train_vec){
  history <- fit(
    object           = model_keras,
    x                = as.matrix(x_train_tbl),
    y                = y_train_vec,
    batch_size       = 32,
    epochs           = 35,
    validation_split = 0.30

  )

  return(history)
}
```

-   The loss is the error of the model.

-   The binnary_accuracy is how accurate is the model. Accuracy is calculated by the ratio of number of correct predicted values over total number of samples.

-   MSE: mean squared error.

-   val_loss: error in the validation dataset. The same is for val_binary_accuracy and val_mse.

-   Epochs is the number of iterations. For each iteration, we may see the model performance. After a number o iterations, we may see the performance getting stable.

```{r}
### Model Fitting & Performance
fit_model(x_train_tbl, y_train_vec)
```

A binary accuracy of 81% is a very good performance for a classification model. Since the validation binary accuracy is close to this value (79%), we may assume that there is no over and underfitting.

Furthermore, associated with the results above, we may see the stabilization of the model performance after the epoch number 10.

## 5.3. Model Performance

To use probability, we stablish a threshold as follows

probability \> threshold -\> 1

probability \< threshold \<- 0

To Machine Learning, this threshold is commonly 0.5%. Thus, we are assuming threshold of 0.5.

-   If your model does multi-class classification (e.g. if it uses a \`softmax\` last-layer activation):

```{=html}
<!-- -->
```
    model %\>% predict(x) %\>% k_argmax()

-   if your model does binary classification (e.g. if it uses a \`sigmoid\` last-layer activation):

```{=html}
<!-- -->
```
    model %\>% predict(x) %\>% \`\>\`(0.5) %\>% k_cast("int32")

```{r}
### Predictions of the model
predict_estimates <- function(model_keras, x_test_tbl, y_test_vec){
  # Predicted Class
  yhat_keras_class_vec <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>% `>`(0.5) %>% k_cast("int32") %>%
  as.vector()
  #yhat_keras_class_vec <- model_keras %>% predict(x = as.matrix(x_test_tbl)) %>% `>`(0.5) %>% k_cast("int32") %>%
    #as.vector()

  # Predicted Class Probability
  yhat_keras_prob_vec  <- predict(object = model_keras, x = as.matrix(x_test_tbl)) %>% `>`(0.5) %>% k_cast("int32") %>%
  as.vector()
  
  # Format test data and predictions for yardstick metrics
  estimates_keras_tbl <- tibble(
    truth      = factor(y_test_vec, levels = c(0, 1)),
    estimate   = factor(yhat_keras_class_vec, levels = c(0, 1)),
    class_prob = yhat_keras_prob_vec 
  
  )

  estimates_keras_tbl
}
```

```{r}
### Predictions from model on test data
estimates_keras_tbl <- predict_estimates(model_keras, x_test_tbl, y_test_vec)
```

```{r}
### Judging model benchmark metrics
# Confusion Table
estimates_keras_tbl %>% conf_mat(truth, estimate)
```

The model was able to predict 914 true negative (no) and 200 true positive (yes) values.

```{r}
# AUC/ROC
estimates_keras_tbl %>% roc_auc(truth, class_prob)
```

Recall shows us how our positive values (churn) are corrected predicted.

```{r}
# Recall for positive label
#tibble(
#  recall = estimates_keras_tbl %>% recall(truth, estimate)
#)

Recall(y_test_vec, ifelse(yhat_keras_class_vec > .5, 1, 0), positive = 1)
```

```{r}
#F1-Score
estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
```

```{r}
### Correlation Analysis ###
coorelation_analysis <- function (x_train_tbl,y_train_vec){


  # Feature correlations to Churn
  corrr_analysis <- x_train_tbl %>%
    mutate(Churn = y_train_vec) %>%
    correlate() %>%
    focus(Churn) %>%
    mutate(feature = colnames(x_train_tbl)) %>%
    arrange(abs(Churn)) %>%
    mutate(feature = as_factor(feature))
  corrr_analysis

  # Correlation visualization
  corrr_analysis %>%
    ggplot(aes(x = Churn, y = fct_reorder(feature, desc(Churn)))) +
    geom_point() +
    # Positive Correlations - Contribute to churn
    geom_segment(aes(xend = 0, yend = feature),
                 color = palette_light()[[2]],
                 data = corrr_analysis %>% filter(Churn > 0)) +
    geom_point(color = palette_light()[[2]],
               data = corrr_analysis %>% filter(Churn > 0)) +
    # Negative Correlations - Prevent churn
    geom_segment(aes(xend = 0, yend = feature),
                 color = palette_light()[[1]],
                 data = corrr_analysis %>% filter(Churn < 0)) +
    geom_point(color = palette_light()[[1]],
               data = corrr_analysis %>% filter(Churn < 0)) +
    # Vertical lines
    geom_vline(xintercept = 0, color = palette_light()[[5]], size = 1, linetype = 2) +
    geom_vline(xintercept = -0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
    geom_vline(xintercept = 0.25, color = palette_light()[[5]], size = 1, linetype = 2) +
    # Aesthetics
    theme_tq() +
    labs(title = "Churn Correlation Analysis",
         subtitle = paste("Positive Correlations (contribute to churn),",
                          "Negative Correlations (prevent churn)"),
         y = "Feature Importance")
}
```

```{r}
### Correlation Insights
coorelation_analysis(x_train_tbl,y_train_vec)
```

Considering a value of correlation to churn equals or higher than the absolute of 0.2.

-   Variables from Contract_Two.year until tenure_bin6 contribute very much to the model and users with this characteristics are less likely to churn (negative correlation values \<= -0.2).

-   The variables PaymentMethod_Electronic.check, InternetService_Fiber.optic and MonthlyCharges also contributes very much to the model - and they are more likely to churn (positive correlation values \>= 0.2).

By using these insights, the business team may decide how characteristics must aim for preventing churn.

# 6. Deployment

```{r}
# save the model in desired location
save_model_hdf5(model_keras, 'user_churn_model.hdf5')
```

```{r}
# Load model
load_model_hdf5("/Users/lucasquemelli/Documents/repos/telecom_customer_churn/user_churn_model.hdf5", custom_objects = NULL, compile = TRUE)
```

# 
